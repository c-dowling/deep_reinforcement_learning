{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hill Climbing\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will implement a selection of simple policy-based methods with OpenAI Gym's Cartpole environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "s_size = len(env.observation_space.high)\n",
    "a_size = env.action_space.n\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with the environment\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, pop_size=1, gamma=1, noise_scale=1e-2, noise_update=0.25, elite_frac=0.2, stoch_act=False, method=\"vanilla\"):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            pop_size (int): number of new weights to compare each episode\n",
    "            gamma (float): discount parameter\n",
    "            noise_scale (int): amount of gaussian noise to add when updating weights\n",
    "            noise_update (float): percentage to increase/decrease noise scale by (sim anneal and adapt noise_scale)\n",
    "            elite_frac (float): percentage of top performers to use in update\n",
    "            stoch_act (bool): if \"True\" will enable stochastic outputs for action values\n",
    "            method (string):        \"vanilla\" = vanilla hill climbing\n",
    "                                    \"cross_ent\" = cross entropy\n",
    "                                    \"steep_asc\" = steepest ascent\n",
    "                                    \"evolution\" = evolution strategy\n",
    "                                    \"adapt_noise_scale\" = adaptive noise scaling\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.gamma = gamma\n",
    "        self.noise_scale = noise_scale\n",
    "        self.noise_update = noise_update\n",
    "        self.n_elite = int(pop_size*elite_frac)\n",
    "        self.stoch_act = stoch_act\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.pop_size = pop_size\n",
    "        self.best_R = 0\n",
    "        self.best_w = 1e-4*np.random.rand(state_size, action_size)\n",
    "        self.new_w = 1e-4*np.random.rand(pop_size,state_size,action_size)\n",
    "\n",
    "        # Raise a warning if an invalid method type is selected\n",
    "        self.method_types = [\"vanilla\", \"steep_asc\", \"sim_anneal\", \"cross_ent\", \"evolution\", \"adapt_noise_scale\"]\n",
    "        if self.method not in self.method_types:\n",
    "            raise ValueError(\"Invalid sim type. Expected one of: %s\" % self.method_types)\n",
    "        if self.method == \"vanilla\" and self.pop_size > 1:\n",
    "            raise ValueError(\"Invalid population size. For vanilla hill climbing 'pop_size' must equal 1\")\n",
    "        elif self.method != \"vanilla\" and self.pop_size == 1:\n",
    "            raise ValueError(f\"Cannot perform {self.method} with population size of 1. To learn with only one set of weights, select 'vanilla' for method.\")\n",
    "\n",
    "    def forward(self, state, n):\n",
    "        \"\"\"Completes a forward pass through network.\"\"\"\n",
    "        x = np.dot(state, self.new_w[n])\n",
    "        return np.exp(x)/sum(np.exp(x))\n",
    "\n",
    "    def act(self, state, n):\n",
    "        \"\"\"Returns the probability that an agent selects each action.\"\"\"\n",
    "        probs = self.forward(state, n)\n",
    "        if self.stoch_act:\n",
    "            action = np.random.choice(self.action_size, p=probs) # option 1: stochastic policy\n",
    "        else:\n",
    "            action = np.argmax(probs)              # option 2: deterministic policy\n",
    "        return action\n",
    "\n",
    "    def simulate_episode(self, max_t, initial_state, new_pop_size=None):\n",
    "        \"\"\"Simulates an episode from an initial_state and returns the rewards for that episode at each timestep.\"\"\"\n",
    "        pop_size = self.pop_size if new_pop_size is None else new_pop_size\n",
    "\n",
    "        rewards = np.zeros((pop_size,max_t))\n",
    "        for n in range(pop_size):\n",
    "            env.reset()\n",
    "            state = initial_state\n",
    "            env.env.state = state\n",
    "            for t in range(max_t):\n",
    "                action = self.act(state, n)\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                rewards[n][t] = reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "        return rewards\n",
    "\n",
    "    def apply_discounts(self, rewards, max_t, new_pop_size=None):\n",
    "        \"\"\"Returns a discounted sum of the rewards.\"\"\"\n",
    "        pop_size = self.pop_size if new_pop_size is None else new_pop_size\n",
    "        discounts = [self.gamma**i for i in range(max_t)]\n",
    "        discounted_rewards = np.empty((pop_size,max_t))\n",
    "        for n in range(pop_size):\n",
    "            discounted_rewards[n] = [a*b for a,b in zip(discounts, rewards[n])]\n",
    "        summed_discounted_rewards = np.sum(discounted_rewards,axis=1)\n",
    "\n",
    "        return summed_discounted_rewards\n",
    "\n",
    "    def update(self, Rs, max_t, initial_state):\n",
    "        \"\"\"Updates the best reward and best weights. Then sets the new weights for the next episode.\"\"\"\n",
    "        if self.method == \"vanilla\":\n",
    "            # If new rewards are better then the best weights update the weights\n",
    "            if Rs >= self.best_R:\n",
    "                self.best_R = Rs\n",
    "                self.best_w = self.new_w\n",
    "            self.new_w = self.best_w + (self.noise_scale * np.random.rand(*self.best_w.shape))\n",
    "\n",
    "        elif self.method in (\"steep_asc\", \"sim_anneal\", \"adapt_noise_scale\"):\n",
    "            new_best_R_idx = np.argmax(Rs)  # Get the index for the best rewards\n",
    "            if Rs[new_best_R_idx] >= self.best_R:\n",
    "                self.best_R = Rs[new_best_R_idx]\n",
    "                self.best_w = self.new_w[new_best_R_idx]\n",
    "                if self.method in (\"sim_anneal\", \"adapt_noise_scale\"):\n",
    "                    # Decrease the noise perimeter if our rewards improved\n",
    "                    self.noise_scale = self.noise_scale * (1 - self.noise_update)\n",
    "            else:\n",
    "                if self.method == \"adapt_noise_scale\":\n",
    "                    # Increase our noise perimeter if our agent has stalled\n",
    "                    self.noise_scale = self.noise_scale * (1 + self.noise_update)\n",
    "            for n in range(self.pop_size):\n",
    "                self.new_w[n] = self.best_w + (self.noise_scale * np.random.rand(*self.best_w.shape))\n",
    "\n",
    "        elif self.method == \"cross_ent\":\n",
    "            # Index our cumulative reward values and sort in descending order\n",
    "            ranked_R_idx = np.argsort(-1*Rs)\n",
    "            # Select the best scores and their corresponding weights\n",
    "            elite_R_idx = ranked_R_idx[:self.n_elite]\n",
    "            elite_ws = [self.new_w[idx,:,:] for idx in elite_R_idx]\n",
    "            # Average the best weights and find their corresponding reward\n",
    "            self.new_w = np.array(elite_ws).mean(axis=0)\n",
    "            self.new_w = np.expand_dims(self.new_w,axis=0)\n",
    "\n",
    "            # Run a simulation with our best averaged weights\n",
    "            new_best_R = self.simulate_episode(max_t,initial_state,new_pop_size=1)\n",
    "            new_best_discount_R = self.apply_discounts(new_best_R, max_t, new_pop_size=1)\n",
    "\n",
    "            # Compare our new reward with our best reward and update weights\n",
    "            if new_best_discount_R >= self.best_R:\n",
    "                self.best_R = new_best_discount_R\n",
    "                self.best_w = self.new_w.squeeze()\n",
    "            self.new_w = np.zeros((self.pop_size,self.state_size,self.action_size))\n",
    "            for n in range(self.pop_size):\n",
    "                self.new_w[n] = self.best_w + (self.noise_scale * np.random.rand(*self.best_w.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 134.00\n",
      "Episode 2\tAverage Score: 167.00\n",
      "Episode 3\tAverage Score: 178.00\n",
      "Episode 4\tAverage Score: 183.50\n",
      "Episode 5\tAverage Score: 186.80\n",
      "Episode 6\tAverage Score: 189.00\n",
      "Episode 7\tAverage Score: 190.57\n",
      "Episode 8\tAverage Score: 191.75\n",
      "Episode 9\tAverage Score: 192.67\n",
      "Episode 10\tAverage Score: 193.40\n",
      "Episode 11\tAverage Score: 194.00\n",
      "Episode 12\tAverage Score: 194.50\n",
      "Episode 13\tAverage Score: 194.92\n",
      "Episode 14\tAverage Score: 195.29\n",
      "Environment solved in 14 episodes!\tAverage Score: 195.29\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(s_size, a_size, method=\"cross_ent\", pop_size=6, elite_frac=0.2)\n",
    "\n",
    "def hill_climbing(n_episodes=1000, max_t=1000, print_every=1):\n",
    "    \"\"\"Implementation of hill climbing with adaptive noise scaling.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        gamma (float): discount rate\n",
    "        print_every (int): how often to print average score (over last 100 episodes)\n",
    "        noise_scale (float): standard deviation of additive noise\n",
    "    \"\"\"\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        initial_state = env.reset()   # Sets the initial state so that the agent can revisit it with different weights.\n",
    "        rewards = agent.simulate_episode(max_t,initial_state)\n",
    "        discounted_rewards = agent.apply_discounts(rewards,max_t)\n",
    "        agent.update(discounted_rewards, max_t, initial_state)\n",
    "\n",
    "        scores_deque.append(agent.best_R)\n",
    "        scores.append(agent.best_R)\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque)>=195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            #agent.policy.w = agent.best_w\n",
    "            break\n",
    "\n",
    "    return scores\n",
    "\n",
    "scores = hill_climbing()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plot the Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Watch a Smart Agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "for t in range(200):\n",
    "    action = agent.act(state, 0)\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break \n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}